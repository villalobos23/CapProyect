---
title: "Exploratory Data Analysis of HC Corpora - Coursera Capstone Proyect"
author: "Luis J. Villalobos"
date: "September 1, 2016"
output: html_document
---
```{r setup,message=FALSE, include=FALSE}
source("fileReading.R", local=environment())
source("exploratoryAnalysis.R", local = environment())
require(tm)
require(RWeka)
require(Rgraphviz)
require(ggplot2)
```
#Introduction


##Initial Analysis

[//]: # ```{r cache=TRUE}
[//]: # downloadZipFile()
[//]: # extractFiles("Coursera-SwiftKey.zip")
[//]: # ```

```{r cache=TRUE}
en_US.blog.file <- "final/en_US/en_US.blogs.txt"
en_US.twitter.file <- "final/en_US/en_US.twitter.txt"
en_US.news.file = "final/en_US/en_US.news.txt"
#en_US.blog.lines <- getNumLines(en_US.blog.file)
#en_US.twitter.lines <- getNumLines(en_US.twitter.file)
#en_US.news.lines <- getNumLines(en_US.news.file)
sampleSize <- 100
en_US.blog.sample <- readNLines(en_US.blog.file,sampleSize)
en_US.twitter.sample <- readNLines(en_US.twitter.file,sampleSize)
en_US.news.sample <- readNLines(en_US.news.file,sampleSize)
```

where is the data from.

space and sizes

head structure

"Corpus" is a collection of text documents.

```{r}
initialCorpus <- createCorpus(c(en_US.news.sample,en_US.twitter.sample,en_US.blog.sample))
```

##Cleaning Data

tokenization for single words
```{r cache=TRUE}
#One word relationships
cleanSample <- cleanCorpus(initialCorpus)
cleanTdm <- createTermDocumentMatrix(cleanSample,1,1,FALSE)
bigramTdm <- createTermDocumentMatrix(cleanSample,2,2,FALSE)
trigramTdm <- createTermDocumentMatrix(cleanSample,3,3,FALSE)
#with stopwords
cleanSampleSW <- cleanCorpus(initialCorpus,removeSW = FALSE)
bigramTdmSW <- createTermDocumentMatrix(cleanSampleSW,2,2,FALSE)
trigramSW <- createTermDocumentMatrix(cleanSampleSW,3,3,FALSE)
#With stemming
cleanSample.stemmed <- stemCorpus(cleanSample)
cleanTdm.stemmed <- createTermDocumentMatrix(cleanSample.stemmed,1,1,FALSE)
```



and profanity filtering

and creation of stopword and non stop n-grams (1-gram, 2-grams, 3-gram)


##Word Frequency and word pairs

###Distribution of word frequencies

```{r}
cleanTdm.freq <- createFrequencyMatrix(cleanTdm)
bigramTdm.freq <- createFrequencyMatrix(bigramTdm)
trigramTdm.freq <- createFrequencyMatrix(trigramTdm)
trigramSW.freq <- createFrequencyMatrix(trigramSW)

print(head(cleanTdm.freq,10))

cleanTdm.fplot <- createFrequencyPlot(cleanTdm.freq,useMean=FALSE, minFreq = 10)
bigramTdm.fplot <- createFrequencyPlot(bigramTdm.freq,useMean = FALSE)
trigramTdm.fplot <- createFrequencyPlot(trigramTdm.freq, useMean = FALSE)
trigramSw.fplot <- createFrequencyPlot(trigramSW.freq, useMean = FALSE)
print(cleanTdm.fplot) 
```



non stopword and stopwords


###Distribution of 2-grams and 3-grams
mixed only

###Word coverage

How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language and with 90%



