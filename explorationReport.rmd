---
title: "Exploratory Data Analysis of HC Corpora - Coursera Capstone Proyect"
author: "Luis J. Villalobos"
date: "September 1, 2016"
output: html_document
---
```{r setup,message=FALSE, include=FALSE}
source("fileReading.R", local=environment())
source("exploratoryAnalysis.R", local = environment())
require(tm)
require(RWeka)
require(Rgraphviz)
require(tau)
require(qdap)
require(NLP)
```
#Introduction


##Initial Analysis

[//]: # ```{r cache=TRUE}
[//]: # downloadZipFile()
[//]: # extractFiles("Coursera-SwiftKey.zip")
[//]: # ```

```{r}
en_US.blog.file <- "final/en_US/en_US.blogs.txt"
en_US.blog.lines <- getNumLines(en_US.blog.file)
en_US.twitter.file <- "final/en_US/en_US.twitter.txt"
en_US.twitter.lines <- getNumLines(en_US.twitter.file)
en_US.blog.sample <- readNLines(en_US.blog.file,10000)
en_US.twitter.sample <- readNLines(en_US.twitter.file,10000)
en_US.news.file = "final/en_US/en_US.news.txt"
en_US.news.lines <- getNumLines(en_US.news.file)
en_US.news.sample <- readNLines(en_US.news.file,10000)
```

where is the data from.

space and sizes

head structure

"Corpus" is a collection of text documents.



##Cleaning Data

tokenization



and profanity filtering

and creation of stopword and non stop n-grams (1-gram, 2-grams, 3-gram)


##Word Frequency and word pairs

###Distribution of word frequencies
non stopword and stopwords


###Distribution of 2-grams and 3-grams
mixed only

###Word coverage

How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language and with 90%



