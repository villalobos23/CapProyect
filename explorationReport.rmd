---
title: "Exploratory Data Analysis of HC Corpora - Coursera Capstone Proyect"
author: "Luis J. Villalobos"
date: "September 1, 2016"
output: html_document
---
```{r setup,message=FALSE, include=FALSE}
source("fileReading.R", local=environment())
source("exploratoryAnalysis.R", local = environment())
require(tm)
require(RWeka)
require(Rgraphviz)
require(ggplot2)
```
#Introduction


##Initial Analysis

[//]: # ```{r cache=TRUE}
[//]: # downloadZipFile()
[//]: # extractFiles("Coursera-SwiftKey.zip")
[//]: # ```

```{r cache=TRUE}
sampleSize <- 100
en_US.blog.file <- "final/en_US/en_US.blogs.txt"
en_US.twitter.file <- "final/en_US/en_US.twitter.txt"
en_US.news.file = "final/en_US/en_US.news.txt"
#en_US.blog.lines <- getNumLines(en_US.blog.file)
#en_US.twitter.lines <- getNumLines(en_US.twitter.file)
#en_US.news.lines <- getNumLines(en_US.news.file)
en_US.blog.sample <- readNLines(en_US.blog.file,sampleSize)
en_US.twitter.sample <- readNLines(en_US.twitter.file,sampleSize)
en_US.news.sample <- readNLines(en_US.news.file,sampleSize)
```

where is the data from.

space and sizes

head structure

"Corpus" is a collection of text documents.

```{r}
initialCorpus <- createCorpus(c(en_US.news.sample,en_US.twitter.sample,en_US.blog.sample))
```

##Cleaning Data

tokenization for single words
```{r cache=TRUE}
cleanSample <- cleanCorpus(initialCorpus)
cleanTdm <- createTermDocumentMatrix(cleanSample,1,1,FALSE)
cleanSampleSW <- cleanCorpus(initialCorpus,removeSW = FALSE)
bigramTdm <- createTermDocumentMatrix(cleanSampleSW,2,2,FALSE)
trigram <- createTermDocumentMatrix(cleanSampleSW,3,3,FALSE)
cleanSample.stemmed <- stemCorpus(cleanSample)
cleanTdm.stemmed <- createTermDocumentMatrix(cleanSample.stemmed,1,1,FALSE)
```



and profanity filtering

and creation of stopword and non stop n-grams (1-gram, 2-grams, 3-gram)


##Word Frequency and word pairs

###Distribution of word frequencies

```{r}
cleanTdm.freq <- as.matrix(cleanTdm)
cleanTdm.freq <- sort(rowSums(cleanTdm.freq),decreasing = T)
cleanTdm.freq <- data.frame(word = names(cleanTdm.freq),freq = cleanTdm.freq)

print(head(cleanTdm.freq,10))

p <- ggplot(subset(cleanTdm.freq, freq>10), aes(word, freq))    
p <- p + geom_bar(stat="identity")   
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1))   
print(p) 
```



non stopword and stopwords


###Distribution of 2-grams and 3-grams
mixed only

###Word coverage

How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language and with 90%



